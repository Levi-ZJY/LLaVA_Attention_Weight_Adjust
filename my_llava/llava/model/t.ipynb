{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2189527-d2f6-437b-a398-5f8b54056f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340a966-609f-460a-a15f-dea2b5d6ed30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0081299a-ba0c-47df-ae26-b465186ccaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-13 00:29:02.787578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736728142.809409    9062 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736728142.816235    9062 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import LlamaModel, LlamaForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78462ca9-e8af-4a70-a587-a1a80f21d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def __init__(self, config):\n",
      "        super().__init__(config)\n",
      "        self.model = LlamaModel(config)\n",
      "        self.vocab_size = config.vocab_size\n",
      "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
      "\n",
      "        # Initialize weights and apply final processing\n",
      "        self.post_init()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LlamaForCausalLM.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad79cfe-74a3-463f-991a-92975c3362b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e67ef28-d8b3-4025-b13d-7773b724d5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaAttention(nn.Module):\n",
      "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
      "\n",
      "    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n",
      "        super().__init__()\n",
      "        self.config = config\n",
      "        self.layer_idx = layer_idx\n",
      "        if layer_idx is None:\n",
      "            logger.warning_once(\n",
      "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
      "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
      "                \"when creating this class.\"\n",
      "            )\n",
      "\n",
      "        self.attention_dropout = config.attention_dropout\n",
      "        self.hidden_size = config.hidden_size\n",
      "        self.num_heads = config.num_attention_heads\n",
      "        self.head_dim = self.hidden_size // self.num_heads\n",
      "        self.num_key_value_heads = config.num_key_value_heads\n",
      "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
      "        self.max_position_embeddings = config.max_position_embeddings\n",
      "        self.rope_theta = config.rope_theta\n",
      "        self.is_causal = True\n",
      "\n",
      "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
      "            raise ValueError(\n",
      "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
      "                f\" and `num_heads`: {self.num_heads}).\"\n",
      "            )\n",
      "\n",
      "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
      "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
      "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
      "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
      "        self._init_rope()\n",
      "\n",
      "    def _init_rope(self):\n",
      "        if self.config.rope_scaling is None:\n",
      "            self.rotary_emb = LlamaRotaryEmbedding(\n",
      "                self.head_dim,\n",
      "                max_position_embeddings=self.max_position_embeddings,\n",
      "                base=self.rope_theta,\n",
      "            )\n",
      "        else:\n",
      "            scaling_type = self.config.rope_scaling[\"type\"]\n",
      "            scaling_factor = self.config.rope_scaling[\"factor\"]\n",
      "            if scaling_type == \"linear\":\n",
      "                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n",
      "                    self.head_dim,\n",
      "                    max_position_embeddings=self.max_position_embeddings,\n",
      "                    scaling_factor=scaling_factor,\n",
      "                    base=self.rope_theta,\n",
      "                )\n",
      "            elif scaling_type == \"dynamic\":\n",
      "                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n",
      "                    self.head_dim,\n",
      "                    max_position_embeddings=self.max_position_embeddings,\n",
      "                    scaling_factor=scaling_factor,\n",
      "                    base=self.rope_theta,\n",
      "                )\n",
      "            else:\n",
      "                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
      "\n",
      "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
      "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
      "            query_slices = self.q_proj.weight.split(\n",
      "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
      "            )\n",
      "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
      "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
      "\n",
      "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            query_states = torch.cat(query_states, dim=-1)\n",
      "\n",
      "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            key_states = torch.cat(key_states, dim=-1)\n",
      "\n",
      "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            value_states = torch.cat(value_states, dim=-1)\n",
      "\n",
      "        else:\n",
      "            query_states = self.q_proj(hidden_states)\n",
      "            key_states = self.k_proj(hidden_states)\n",
      "            value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            if self.layer_idx is None:\n",
      "                raise ValueError(\n",
      "                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n",
      "                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n",
      "                    \"with a layer index.\"\n",
      "                )\n",
      "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n",
      "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
      "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
      "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
      "        else:\n",
      "            attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 获取函数的源代码\n",
    "print(inspect.getsource(LlamaAttention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9cb556-0075-466a-92f3-08c7342e451b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaDecoderLayer(nn.Module):\n",
      "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.hidden_size = config.hidden_size\n",
      "\n",
      "        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
      "\n",
      "        self.mlp = LlamaMLP(config)\n",
      "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
      "            attention_mask (`torch.FloatTensor`, *optional*):\n",
      "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
      "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more detail.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      "                (see `past_key_values`).\n",
      "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
      "        \"\"\"\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LlamaDecoderLayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d7fd0c-e6cd-4beb-abda-6b17a0a61a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@add_start_docstrings(\n",
      "    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n",
      "    LLAMA_START_DOCSTRING,\n",
      ")\n",
      "class LlamaModel(LlamaPreTrainedModel):\n",
      "    \"\"\"\n",
      "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
      "\n",
      "    Args:\n",
      "        config: LlamaConfig\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, config: LlamaConfig):\n",
      "        super().__init__(config)\n",
      "        self.padding_idx = config.pad_token_id\n",
      "        self.vocab_size = config.vocab_size\n",
      "\n",
      "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
      "        self.layers = nn.ModuleList(\n",
      "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
      "        )\n",
      "        self._use_sdpa = config._attn_implementation == \"sdpa\"\n",
      "        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n",
      "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "\n",
      "        self.gradient_checkpointing = False\n",
      "        # Initialize weights and apply final processing\n",
      "        self.post_init()\n",
      "\n",
      "    def get_input_embeddings(self):\n",
      "        return self.embed_tokens\n",
      "\n",
      "    def set_input_embeddings(self, value):\n",
      "        self.embed_tokens = value\n",
      "\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length = inputs_embeds.shape[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning_once(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        past_key_values_length = 0\n",
      "        if use_cache:\n",
      "            use_legacy_cache = not isinstance(past_key_values, Cache)\n",
      "            if use_legacy_cache:\n",
      "                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
      "            past_key_values_length = past_key_values.get_usable_length(seq_length)\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if self._use_flash_attention_2:\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        elif self._use_sdpa and not output_attentions:\n",
      "            # output_attentions=True can not be supported when using SDPA, and we fall back on\n",
      "            # the manual implementation that requires a 4D causal mask in all cases.\n",
      "            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n",
      "                attention_mask,\n",
      "                (batch_size, seq_length),\n",
      "                inputs_embeds,\n",
      "                past_key_values_length,\n",
      "            )\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
      "            )\n",
      "\n",
      "        # embed positions\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = None\n",
      "\n",
      "        for decoder_layer in self.layers:\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    past_key_values,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_values,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = None\n",
      "        if use_cache:\n",
      "            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LlamaModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91846efc-7d65-4df5-b58d-f92086f3c161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9646bc-5882-4fe6-b351-b276b6d75b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341f383-632f-44a2-a7e4-86b0d40ede36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa50ef2-60b9-4a9d-9ffb-925180857e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bc32e-41d6-45d0-8c3e-27cc90bbb52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48db49-79b2-4d64-81dc-daccace428cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681c42da-09e9-419e-80fa-cf8f63d3c8cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module, class, method, function, traceback, frame, or code object was expected, got builtin_function_or_method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2653/2206281770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 获取 scaled_dot_product_attention 源代码\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     OSError is raised if the source code cannot be retrieved.\"\"\"\n\u001b[0;32m-> 1139\u001b[0;31m     \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m   1120\u001b[0m     \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    938\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;31m# Invalidate cache if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/package/package_importer.py\u001b[0m in \u001b[0;36m_patched_getfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_orig_getfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miscode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     raise TypeError('module, class, method, function, traceback, frame, or '\n\u001b[0m\u001b[1;32m    798\u001b[0m                     'code object was expected, got {}'.format(\n\u001b[1;32m    799\u001b[0m                     type(object).__name__))\n",
      "\u001b[0;31mTypeError\u001b[0m: module, class, method, function, traceback, frame, or code object was expected, got builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "\n",
    "# 获取 scaled_dot_product_attention 源代码\n",
    "print(inspect.getsource(torch.nn.functional.scaled_dot_product_attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc907b94-2db4-4cb5-8d6f-22af5b158453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f3dd1-894a-457c-92b1-1a138520e2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e3a69-8940-40b5-aaf1-3854aa281bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d16a3c8-a717-4e62-9b8b-425a2997b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    3,    4, -200,    6,    2,    7]], device='cuda:0')\n",
      "tensor([[1, 3, 4, 6, 2, 7]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建原始 tensor\n",
    "tensor = torch.tensor([\n",
    "    [1, 3, 4, -200, 6, 2, 7]\n",
    "], device='cuda:0')\n",
    "\n",
    "# 使用布尔索引移除 -200\n",
    "new_tensor = tensor[tensor != -200].view(1, -1)\n",
    "\n",
    "# 打印结果\n",
    "print(tensor)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a08e59c-f266-420e-9de9-5fe0780da710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    3,    4, -200,    6,    2,    7]], device='cuda:0')\n",
      "tensor([[ 1,  3, 11,  6,  2,  7]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "new_tensor[0,2] = 11\n",
    "print(tensor)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92c2d59-2330-493f-8f2a-ba98713c87e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(new_tensor.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704682f-815f-4a18-b93a-e3f973ac6a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff45be-752f-49e6-8dbc-36b60ad21938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47cfe71-d590-4465-919b-e3d5f10bef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = tensor = torch.tensor([2], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c07d305-54b7-4b5a-91b3-2eaef16512ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf779c-2aee-4bc4-8b9b-3e51b02395a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5905dc37-54c7-42df-a9b9-89e6df2ef98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4], device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 3, 4, -200, 6, 2, 7], device='cuda:0')\n",
    "\n",
    "print(tensor[a])\n",
    "print(tensor[a][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce240679-e90f-4177-be00-ef17c19b55cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d24af-b461-4f75-95ee-511060b7d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04a0438-0788-438c-8994-31909e28ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
    "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
    "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
    "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
    "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
    "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
    "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
    "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
    "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
    "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
    "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
    "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
    "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
    "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
    "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
    "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
    "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
    "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
    "         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
    "         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
    "         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
    "         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
    "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
    "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
    "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
    "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
    "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
    "         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
    "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
    "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
    "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
    "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
    "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
    "         504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
    "         518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
    "         532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
    "         546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
    "         560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
    "         574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
    "         588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
    "         602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615,\n",
    "         616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
    "         630, 631, 632, 633, 634, 635, 636, 637]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bbf878-f924-4489-bf6f-ea15cfb4f44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n"
     ]
    }
   ],
   "source": [
    "print(len(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500cb69-7165-45dd-9e91-24e598020512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43008878-8c66-4f9d-9e1d-d4c653624e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0, 4, device='cuda:0').unsqueeze(0)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d43d79-d73a-4dfe-92f8-515a15bdedc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
